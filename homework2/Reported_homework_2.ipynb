{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey, I'm Jobert Gutierrez and hereafter you'll find the logic and code used to answer the second assignment in the program Data Engineering Zoomcamp offered by Data Talks Club."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Module 2 Homework__\n",
    "\n",
    "> In case you don't get one option exactly, select the closest one\n",
    "\n",
    "For the homework, we'll be working with the green taxi dataset located here:\n",
    "\n",
    "https://github.com/DataTalksClub/nyc-tlc-data/releases/tag/green/download\n",
    "\n",
    "## Assignment\n",
    "The goal will be to construct an ETL pipeline that loads the data, performs some transformations, and writes the data to a database (and Google Cloud!).\n",
    "\n",
    "- Create a new pipeline, call it _green_taxi_etl_\n",
    "- Add a data loader block and use Pandas to read data for the final quarter of 2020 (months 10, 11, 12).\n",
    "    - You can use the same datatypes and date parsing methods shown in the course.\n",
    "    - `BONUS`: load the final three months using a for loop and `pd.concat`\n",
    "- Add a transformer block and perform the following:\n",
    "- Remove rows where the passenger count is equal to 0 or the trip distance is equal to zero.\n",
    "- Create a new column lpep_pickup_date by converting lpep_pickup_datetime to a date.\n",
    "- Rename columns in Camel Case to Snake Case, e.g. VendorID to `vendor_id`.\n",
    "- Add three assertions:\n",
    "    - vendor_id is one of the existing values in the column (currently)\n",
    "    - passenger_count is greater than 0\n",
    "    - trip_distance is greater than 0\n",
    "- Using a Postgres data exporter (SQL or Python), write the dataset to a table called green_taxi in a schema mage. Replace the table if it already exists.\n",
    "- Write your data as Parquet files to a bucket in GCP, partioned by lpep_pickup_date. Use the pyarrow library!\n",
    "- Schedule your pipeline to run daily at `5AM UTC`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "### Question 1. Data Loading\n",
    "Once the dataset is loaded, what's the shape of the data?\n",
    "\n",
    "266,855 rows x 20 columns <br>\n",
    "544,898 rows x 18 columns <br>\n",
    "544,898 rows x 20 columns <br>\n",
    "133,744 rows x 20 columns <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer: \n",
    "Using the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import pandas as pd\n",
    "import requests\n",
    "if 'data_loader' not in globals():\n",
    "    from mage_ai.data_preparation.decorators import data_loader\n",
    "if 'test' not in globals():\n",
    "    from mage_ai.data_preparation.decorators import test\n",
    "\n",
    "\n",
    "@data_loader\n",
    "def load_data_from_api(*args, **kwargs):\n",
    "\n",
    "    month_list = [10, 11, 12]\n",
    "    data = list()\n",
    "    \n",
    "    for month in month_list:\n",
    "\n",
    "        url = f'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2020-{month}.csv.gz'\n",
    "        \n",
    "        \n",
    "        #Declaring data types is a goood DE practice coz it reduces memory consumption\n",
    "        taxi_dtypes = {\n",
    "            'VendorID': pd.Int64Dtype(),\n",
    "            'store_and_fwd_flag': str,\n",
    "            'RatecodeID': pd.Int64Dtype(),    \n",
    "            'PULocationID': pd.Int64Dtype(),\n",
    "            'DOLocationID': pd.Int64Dtype(),\n",
    "            'passenger_count': pd.Int64Dtype(),\n",
    "            'trip_distance': float,\n",
    "            'fare_amount': float,\n",
    "            'extra': float,\n",
    "            'mta_tax': float,\n",
    "            'tip_amount': float,\n",
    "            'tolls_amount': float,        \n",
    "            'improvement_surcharge': float,\n",
    "            'total_amount': float,\n",
    "            'payment_type': pd.Int64Dtype(),\n",
    "            'trip_type':pd.Int64Dtype(),\n",
    "            'congestion_surcharge': float,\n",
    "        }\n",
    "        parse_dates = ['lpep_pickup_datetime', 'lpep_dropoff_datetime']\n",
    "\n",
    "        df = pd.read_csv(url, sep=',', compression='gzip', dtype=taxi_dtypes, parse_dates = parse_dates)\n",
    "\n",
    "        data.append(df)\n",
    "\n",
    "    dataframe = pd.concat(data)\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the result of __266855 rows x 20 columns__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2. Data Transformation\n",
    "Upon filtering the dataset where the passenger count is greater than 0 and the trip distance is greater than zero, how many rows are left?\n",
    "\n",
    "544,897 rows <br>\n",
    "266,855 rows <br>\n",
    "139,370 rows <br>\n",
    "266,856 rows <br>\n",
    "\n",
    "## Answer:\n",
    "\n",
    "Using the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from mage_ai.data_cleaner.transformer_actions.base import BaseAction\n",
    "from mage_ai.data_cleaner.transformer_actions.constants import ActionType, Axis\n",
    "from mage_ai.data_cleaner.transformer_actions.utils import build_transformer_action\n",
    "from pandas import DataFrame\n",
    "\n",
    "if 'transformer' not in globals():\n",
    "    from mage_ai.data_preparation.decorators import transformer\n",
    "if 'test' not in globals():\n",
    "    from mage_ai.data_preparation.decorators import test\n",
    "\n",
    "\n",
    "@transformer\n",
    "def execute_transformer_action(df: DataFrame, *args, **kwargs) -> DataFrame:\n",
    "    \n",
    "    # Identifying the number of rows with zero passengers\n",
    "    print(f'Preprocessing: rows with zero passenger count: ', df['passenger_count'].isin([0]).sum())\n",
    "    data = df[df['passenger_count']>0]\n",
    "\n",
    "    # Identifying the number of rows with trip distance of zero \n",
    "    print(f'Preprocessing: rows with trip distance of zero: ', data['trip_distance'].isin([0]).sum())\n",
    "    data = data[data['trip_distance']>0]\n",
    "\n",
    "    # Creating the date 'lpep_pickup_date' column \n",
    "    data['lpep_pickup_date'] = data['lpep_pickup_datetime'].dt.date\n",
    "\n",
    "    # Converting column names from Camel case to Snake case\n",
    "    data.columns = (data.columns\n",
    "                    .str.replace('(?<=[a-z])(?=[A-Z])', '_', regex=True)\n",
    "                    .str.lower())\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "@test\n",
    "def test_vendor_id(output, *args) -> None:\n",
    "    assert \"vendor_id\" in output.columns, 'vendor_id exists in Column names.'\n",
    "\n",
    "@test\n",
    "def test_passenger_count(output, *args) -> None:\n",
    "    assert output['passenger_count'].isin([0]).sum() == 0, 'There are rides with zero passengers.'\n",
    "\n",
    "@test\n",
    "def test_trip_distance(output, *args) -> None:\n",
    "    assert output['trip_distance'].isin([0]).sum() == 0, 'There are rides with trip distance of zero.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the result __139370 rows x 21 columns__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3. Data Transformation\n",
    "Which of the following creates a new column `lpep_pickup_date` by converting `lpep_pickup_datetime` to a date?\n",
    "\n",
    "data = data['lpep_pickup_datetime'].date <br>\n",
    "data('lpep_pickup_date') = data['lpep_pickup_datetime'].date <br>\n",
    "data['lpep_pickup_date'] = data['lpep_pickup_datetime'].dt.date <br>\n",
    "data['lpep_pickup_date'] = data['lpep_pickup_datetime'].dt().date() <br>\n",
    "\n",
    "## Answer:\n",
    "\n",
    "As seen in the code snippet in previous question, the correct asnwer is <br>`data['lpep_pickup_date'] = data['lpep_pickup_datetime'].dt.date`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4. Data Transformation\n",
    "What are the existing values of VendorID in the dataset?\n",
    "\n",
    "1, 2, or 3 <br>\n",
    "1 or 2 <br>\n",
    "1, 2, 3, 4 <br>\n",
    "1 <br>\n",
    "\n",
    "## Answer:\n",
    "\n",
    "Using the code `print(data['vendor_id'].unique())` we see that the __unique values contained into the vendor_id column are [1, 2]__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5. Data Transformation\n",
    "How many columns need to be renamed to snake case?\n",
    "\n",
    "3 <br>\n",
    "6 <br>\n",
    "2 <br>\n",
    "4 <br>\n",
    "\n",
    "## Answer:\n",
    "\n",
    "Based on the small amount of columns (20), we can follow and count by visual inspection. Hence, __only 4 columns__ need to be renamed (VendorID, RatecodeID, PULocationID, DOLocationID)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6. Data Exporting\n",
    "Once exported, how many partitions (folders) are present in Google Cloud?\n",
    "\n",
    "96 <br>\n",
    "56 <br>\n",
    "67 <br>\n",
    "108 <br>\n",
    "\n",
    "## Answer:\n",
    "\n",
    "Using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from mage_ai.settings.repo import get_repo_path\n",
    "from mage_ai.io.config import ConfigFileLoader\n",
    "from mage_ai.io.google_cloud_storage import GoogleCloudStorage\n",
    "from pandas import DataFrame\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "\n",
    "if 'data_exporter' not in globals():\n",
    "    from mage_ai.data_preparation.decorators import data_exporter\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS']= \"/home/src/data-taxi-1-a1d4e91c10cd.json\"\n",
    "\n",
    "project_id = 'data-taxi-1'\n",
    "bucket_name = 'terraform-taxi-data-1'\n",
    "table_name = 'nyc_taxi_data'\n",
    "\n",
    "root_path = f'{bucket_name}/{table_name}'\n",
    "\n",
    "@data_exporter\n",
    "def export_data_to_google_cloud_storage(df: DataFrame, **kwargs) -> None:\n",
    "    \n",
    "    table = pa.Table.from_pandas(df)\n",
    "\n",
    "    gcs = pa.fs.GcsFileSystem()\n",
    "\n",
    "    pq.write_to_dataset(\n",
    "        table,\n",
    "        root_path = root_path,\n",
    "        partition_cols = ['lpep_pickup_date'],\n",
    "        filesystem = gcs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the field `lpep_pickup_date` to create partitions in Google Cloud Storage, __I get 96 folders in my table nyc_taxi_data from the data's upload operation__. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
